{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdd73d9",
   "metadata": {},
   "source": [
    "## RETRIEVER AND CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b915a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('attention.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc46c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "split_text=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=250).split_documents(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af4291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajith\\AppData\\Local\\Temp\\ipykernel_12492\\4092243650.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  db = FAISS.from_documents(split_text,OllamaEmbeddings(model=\"llama3.2:1b\"))\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(split_text,OllamaEmbeddings(model=\"llama3.2:1b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7453ecd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"An estimation function can be described as mapping a query\"\n",
    "result= db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1272f135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajith\\AppData\\Local\\Temp\\ipykernel_12492\\676642843.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:1b\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a5d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template(\"\"\"Answer the following question based on context. <context>{context}<context> Question:{input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b2de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "doc_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69616ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000010E87211BE0>, search_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = db.as_retriever()\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c07966",
   "metadata": {},
   "source": [
    "### query->> retriever->>vector store(FAISS)->>LLM[Prompt]->>response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56b29090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(ret,doc_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6111749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question seems to refer to the way in which the Transformer architecture uses self-attention mechanisms to facilitate communication between different parts of a network, including the encoder and decoder stacks. Specifically, it asks how an attention function can be described.\\n\\nIn the context of the Transformer model, an attention function is used to selectively focus on certain parts of the input sequence when generating output sequences for the decoder. This process allows the model to capture complex relationships between different positions in the input and output sequences.\\n\\nIn essence, an attention function takes in a set of vectors (representing the input) and outputs a set of weights or scores that represent which parts of the input should be attended to for each position in the output sequence. These weights are used to compute the weighted sum of the input vectors at each position, effectively \"attaching\" them to the desired part of the output sequence.\\n\\nIn other words, an attention function is a way of performing multi-step self-attention over sequences, where at each step, it selectively focuses on certain parts of the previous context and computes a weighted average that reflects how likely those parts are to be relevant for the current position in the output sequence.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"How can an attention function can be described \"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf12c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
